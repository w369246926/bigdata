tier2.sources = r1
tier2.sink = k1
tier2.channels = c1


#source
tier2.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
tier2.sources.r1.batchSize = 500
tier2.sources.r1.batchDurationMillis = 2000
tier2.sources.r1.kafka.bootstrap.servers = hadoop104:9092,hadoop105:9092,hadoop106:9092
tier2.sources.r1.kafka.topics = trace
tier2.sources.r1.kafka.consumer.group.id = trace-consumer1
#过滤器
tier2.sources.r1.interceptors = i1
tier2.sources.r1.interceptors.i1.type = cn.ac.iie.dcNew.interceptor.TraceInterceptor$Builder
tier2.sources.r1.selector.type = multiplexing
tier2.sources.r1.selector.header = topic
tier2.sources.r1.selector.mapping.trace = c1

#sink
tier2.sinks.k1.type = hdfs
tier2.sinks.k1.hdfs.path = /trace/%Y%m%d/%H/%{devInfo}/
#上传文件的前缀
tier2.sinks.k1.hdfs.filePrefix = trace%H%M
#是否按照时间滚动文件夹
tier2.sinks.k1.hdfs.round = true
#多少时间单位创建一个新的文件夹
tier2.sinks.k1.hdfs.roundValue = 1
#重新定义时间单位
tier2.sinks.k1.hdfs.roundUnit = hour
#是否使用本地时间戳
tier2.sinks.k1.hdfs.useLocalTimeStamp = true
#积攒多少个 Event 才 flush 到 HDFS 一次
tier2.sinks.k1.hdfs.batchSize = 100
#设置文件类型，可支持压缩
tier2.sinks.k1.hdfs.fileType = DataStream
#多久生成一个新的文件
tier2.sinks.k1.hdfs.rollInterval = 3600
#设置每个文件的滚动大小
tier2.sinks.k1.hdfs.rollSize = 134217700
#文件的滚动与 Event 数量无关
tier2.sinks.k1.hdfs.rollCount = 0
#最小冗余数
tier2.sinks.k1.hdfs.minBlockReplicas = 1

#channel
tier2.channels.c1.type=memory
#tier2.channels.c1.type=file
tier2.channels.c1.capacity = 30000
tier2.channels.c1.transactionCapacity = 10000
#tier2.channels.c1.checkpointDir = /opt/flume-log/flume/checkpoint
#tier2.channels.c1.dataDirs =  /opt//flume-log/flume/data
 
#bind
tier2.sources.r1.channels=c1
tier2.sinks.k1.channel=c1
