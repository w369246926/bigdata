# **千亿级数仓项目第01天讲义**



**课程目标**

* 了解大数据离线数仓架构
* 了解项目部署环境（数据规模和集群规模）
* 掌握ETL工具 Kettle常用组件的使用
* 能够掌握kettle作业与转换区别以及linux部署



## 1. 电商行业简介

### 电商行业分析

近年来，中国的电子商务快速发展，交易额连创新高，电子商务在各领域的应用不断拓展和深化、相关服务业蓬勃发展、支撑体系不断健全完善、创新的动力和能力 不断增强。电子商务正在与实体经济深度融合，进入规模性发展阶段，对经济社会生活的影响不断增大，正成为我国经济发展的新引擎。

中国电子商务研究中心数据显示，截止到 2012 年底，中国电子商务市场交易规模达 7.85万亿人民币，同比增长 30.83%。其中，B2B 电子商务交易额 达 6.25 万亿，同比增长 27%。而 2011 年全年，中国电子商务市场交易额达 6 万亿人民币，同比增长 33%，占 GDP 比重上升到 13%；2012 年，电子商务占 GDP 的比重已经高达 15%。

| 双十一交易额                               |
| ------------------------------------------ |
| ![1574252270084](assets/1574252270084.png) |



**电商行业技术特点**

* 技术新

* 技术范围广

* 分布式

* 高并发、集群、负载均衡(Nginx)、高可用(备机)

* 海量数据

* 业务复杂

* 系统安全

### 电商业务系统简介

品优购网上商城是一个综合性的 B2B2C 平台，类似京东商城、天猫商城。网站采用商家入驻的模式，商家入驻平台提交申请，有平台进行资质审核，审核通过后，商家拥有独立的管理后台录入商品信息。商品经过平台审核后即可发布。

品优购网上商城主要分为

* 网站前台
* 运营商后台
* 商家管理后台

#### 网站前台

主要包括

* 网站首页
* 商家首页
* 商品详细页
* 搜索页
* 会员中心
* 订单与支付相关页面
* 秒杀频道等。

| 前台                                                         |
| ------------------------------------------------------------ |
| ![image-20200205182315407](assets/image-20200205182315407.png) |



#### 运营商后台

是运营商的运营人员的管理后台。 主要包括商家审核、品牌管理、规格管理、模板管理、商品分类管理、商品审核、广告类型管理、广告管理、订单查询、商家结算等。

| 后台                                                         |
| ------------------------------------------------------------ |
| ![image-20200205182239099](assets/image-20200205182239099.png) |



#### 商家管理后台

入驻的商家进行管理的后台，主要功能是对商品的管理以及订单查询统计、资金结算等功能。

| 商家管理后台                                                 |
| ------------------------------------------------------------ |
| ![image-20200205182159505](assets/image-20200205182159505.png) |



## 大数据数仓项目简介

###  业务流程

本次数仓业务流程主要分为两类，

* 一类是用户下单、提交订单、支付、退款这一条线，
* 另一类是我们收集用户的页面行为数据：用户搜索商品、添加购物车 、提交订单、支付订单 的日志数据，分析电商网站常见的PV，UV，GMV，

**GMV (Gross Merchandise Volume)**：主要是指网站的成交金额，而这里的成交金额包括：付款金额和未付款。

千亿级数仓模仿阿里巴巴双十一的大屏显示功能实现的互联网电商指标的离线分析，同时也模仿了阿里巴巴大数据平台上面数据仓库的设计思想和理念。大家通过学习这个项目，能够掌握以下三个核心技能：

1、数据仓库的概念和建设过程

2、离线数据仓库的功能、使用场景和常用的技术栈



###  大数据离线数仓项目架构

| 离线项目架构图                                               |
| ------------------------------------------------------------ |
| ![image-20200205155603370](assets/image-20200205155603370.png) |



### 项目具体技术简介

- Kettle
- 缓慢变化维(拉链表):时间维度，脚本生成，时间维度生成之后不会变化，SCD问题我们使用拉链表来解决；
- Hive
- kettle：导出数据的工具
- Spark SQL：计算引擎
- Kylin：计算引擎，进行预计算之后的多维统计分析可以达到亚秒级别。



###  项目环境介绍

* 业务数据量
  * 用户数：300W
  * 每日订单量：10W
  * 每日交易额：700W
  * 商家数：5W
  * 商品数：45W
  * PV：500W
  * UV：50W
  
* 数据在hdfs中平均每天 40G左右的速度增长，存储3份，每天增长大概120G,存储hive表时

  会说过parquet格式+snappy压缩

* 硬件资源
  * 数量：30台

  * CPU资源：24核

  * 内存：128G

  * 硬盘：4T



##  可视化ETL工具-Kettle入门 

###  Kettle介绍

对于企业或行业应用来说，经常会遇到各种数据的处理，转换，迁移，掌握一种etl工具的使用，必不可少，这里要学习的ETL工具是——Kettle，现在已经更名为**PDI**。

* Kettle是一款国外开源的ETL工具，纯java编写，可以在Window、Linux、Unix上运行，绿色无需安装

* Kettle 中文名称叫水壶，该项目的主程序员MATT 希望把各种数据放到一个壶里，然后以一种指定的格式流出

* Kettle允许管理来自不同数据库的数据，提供一个图形化的用户环境来描述想做什么，无需关心怎么做



**大数据岗位需求**

<img src="assets/1570150180398.png" align="left"/>

<img src="assets/1570150277314.png" align="left">

<img src="assets/1570150336737.png" align="left">

<img src="assets/1570150477695.png" align="left" />



### Kettle安装、配置

环境要求：

* 安装、配置好JDK

1、下载Kettle

* `资料\安装包\pdi-ce-8.2.0.0-342.zip`

2、解压Kettle

| 解压后目录                                                   |
| ------------------------------------------------------------ |
| ![image-20200205162007791](assets/image-20200205162007791.png) |



3、双击spoon.bat 启动spoon

| 启动脚本                                                     |
| ------------------------------------------------------------ |
| ![image-20200205162045664](assets/image-20200205162045664.png) |
| 启动后结果如图                                               |
| ![image-20200205163706754](assets/image-20200205163706754.png) |

#### Kettle入门案例

需求：

* 把数据从CSV文件（ketttle测试数据\用户数据源\user.csv）抽取到Excel文件

数据源：

| id                 | name | age  | gender | province | city   | region   | phone       | birthday   | hobby              | 注册时间            |
| ------------------ | ---- | ---- | ------ | -------- | ------ | -------- | ----------- | ---------- | ------------------ | ------------------- |
| 392456197008193000 | 张三 | 20   | 0      | 北京市   | 昌平区 | 回龙观   | 18589407692 | 1970-08-19 | 美食;篮球;足球     | 2018-08-06 09:44:43 |
| 267456198006210000 | 李四 | 25   | 1      | 河南省   | 郑州市 | 郑东新区 | 18681109672 | 1980-06-21 | 音乐;阅读;旅游     | 2017-04-07 09:14:13 |
| 892456199007203000 | 王五 | 24   | 1      | 湖北省   | 武汉市 | 汉阳区   | 18798009102 | 1990-07-20 | 写代码;读代码;算法 | 2016-06-08 07:34:23 |
| 492456198712198000 | 赵六 | 26   | 2      | 陕西省   | 西安市 | 莲湖区   | 18189189195 | 1987-12-19 | 购物;旅游          | 2016-01-09 19:15:53 |
| 392456197008193000 | 张三 | 20   | 0      | 北京市   | 昌平区 | 回龙观   | 18589407692 | 1970-08-19 | 美食;篮球;足球     | 2018-08-06 09:44:43 |
| 392456197008193000 | 张三 | 20   | 0      | 北京市   | 昌平区 | 回龙观   | 18589407692 | 1970-08-19 | 美食;篮球;足球     | 2018-08-06 09:44:43 |



实现步骤：

1、在Kettle中新建转换

2、拖拽一个CSV输入组件、一个Excel输出组件、并按住Shift拖动鼠标连接两个组件

3、配置CSV输入组件、Excel输出组件



具体实现：

1、新建转换

| 新建转换                                                     |
| ------------------------------------------------------------ |
| ![image-20200109104022621](assets/image-20200109104022621.png) |



2、拖拽一个CSV输入组件、一个Excel输出组件、并按住Shift拖动鼠标连接两个组件

| 组件架构图                                 |
| ------------------------------------------ |
| ![1570240867723](assets/1570240867723.png) |



3、配置CSV输入组件

* 选择要进行导入的CSV数据源
* 点击 「获取字段」，读取CSV中的列
* 点击  「预览」，浏览CSV中的数据

![1570328488039](assets/1570328488039.png)

![1570328569580](assets/1570328569580.png)

4、配置Excel组件

| 指定输出Excel文件的位置                             |
| --------------------------------------------------- |
| <img src="assets/1570328608969.png" align="left" /> |



5、点击 三角形 箭头执行

| 执行                                                         |
| ------------------------------------------------------------ |
| <img src="assets/1570328732887.png" align="left" style="border:1px solid #999"/> |





#### Kettle数据流结构图

| kettle数据流结构图                         |
| ------------------------------------------ |
| ![1570332641151](assets/1570332641151.png) |





##  Kettle输入/输出组件

###  输入组件

####  JSON数据文件输入

需求：

- 将 `资料\kettle测试数据\用户数据源\user.json` 数据文件，通过Kettle，抽取到Excel中

  | json数据格式                               |
  | ------------------------------------------ |
  | ![1570330332193](assets/1570330332193.png) |

  





操作步骤：

1、新建转换

2、拽入 JSON input组件、Microsoft Excel输出组件、并连接两个组件

| 组件架构图                                                   |
| ------------------------------------------------------------ |
| <img src="assets/1569600093587.png" align="left" style="border:1px solid #999" /> |



3、配置 JSON input 组件

| ① 指定JSON文件数据源                                |
| --------------------------------------------------- |
| ![1570330483490](assets/1570330483490.png)          |
| ② 选择 JSON 字段                                    |
| ![1570330514553](assets/1570330514553.png)          |
| 3、配置 Excel 输出 组件                             |
| <img src="assets/1570330579774.png" align="left" /> |

5、启动执行



#### 表输入

需求：

- 将MySQL数据库中的 user 表中的数据抽取到Excel文件中

环境准备：

一、Kettle整合MySQL数据库

1、将资料中的 MySQL jdbc 驱动包导入到 pdi-ce-8.2.0.0-342\data-integration\lib 中

2、重启 Kettle



二、MySQL建库

1、导入 `资料\kettle测试数据\用户数据源\test_t_user.sql` 到 MySQL数据库中

| 准备测试数据源                                      |
| --------------------------------------------------- |
| <img src="assets/1570412048454.png" align="left" /> |





实现步骤：

1、拉动 输入/表输入 、输出/Excel输出 组件、连接两个组件

2、配置表输入

3、配置Excel输出组件



具体操作：

1、拉动 输入/表输入 、输出/Excel输出 组件、连接两个组件

2、配置表输入

2.1 新建数据库连接

| kettle新建数据库连接                       |
| ------------------------------------------ |
| ![1569633093678](assets/1569633093678.png) |



2.2 选择 t_user 表，并获取SQL查询语句

| 编写sql语句                                |
| ------------------------------------------ |
| ![1570331859373](assets/1570331859373.png) |



2.3 预览数据

| 预览数据                                   |
| ------------------------------------------ |
| ![1570331893525](assets/1570331893525.png) |

**注意：创建一个共享的数据库连接操作步骤**

| 主对象树--》DB连接                                           |
| ------------------------------------------------------------ |
| ![image-20200220143959173](assets/image-20200220143959173.png) |



| 新建连接                                                     |
| ------------------------------------------------------------ |
| ![image-20200220144039392](assets/image-20200220144039392.png) |



| 设置连接共享                                                 |
| ------------------------------------------------------------ |
| ![image-20200220144227459](assets/image-20200220144227459.png) |

鼠标右键--》共享；字体变粗后该连接可以全局使用！

3、配置Excel输出组件

* 指定Excel输出位置

  | 指定excel输出位置                                   |
  | --------------------------------------------------- |
  | <img src="assets/1570332047436.png" align="left" /> |

  

####  生成记录

数据仓库中绝大多数的数据都是业务系统生成的动态数据，但是其中一部分维度数据不是动态的，比如：日期维度。静态维度数据就可以提前生成。

需求：

- 往 Excel 文件中插入1000条记录：id为1，name为itheima，age为18

操作步骤：

1、拖入 输入/生成记录 组件、输出/Excel输出 组件、连接两个组件

2、配置生成记录组件

3、配置Excel输出

具体实现：

1、拖入 输入/生成记录 组件、输出/Excel输出 组件、连接两个组件

| 组件配置图                                                   |
| ------------------------------------------------------------ |
| <img src="assets/1569633552688.png" align="left" style="border:1px solid #999"/> |



2、配置生成记录组件

| 生成记录组件                               |
| ------------------------------------------ |
| ![1569633537293](assets/1569633537293.png) |



### 输出组件

#### 文本文件输出

需求：

- 从mysql数据库的test库的t_user表 抽取数据到文本文件中

步骤：

1、拖入 一个 输入/表输入、一个 输出/文本文件输出、并连接两个组件

| 组件配置图                                                   |
| ------------------------------------------------------------ |
| <img src="assets/1569634209629.png" align="left" style="border:1px solid #999" > |



2、指定 从哪个表中获取数据

3、指定表中的数据输出到哪个文件

#### 表输出

* 表输出就是把数据写入指定的表

需求：

- 从 `资料\kettle测试数据\用户数据源\user.json`中读取id, name, age字段的数据，
- 装载到mysql数据库的 t_user_1 表中

操作步骤：

1、拖动 输入/JSON Input组件 ，输出/表输出，连接两个组件

2、JSON输入配置

3、表输出配置

具体操作：

1、拖动 输入/JSON Input组件 ，输出/表输出，连接两个组件

| 组件配置图                                 |
| ------------------------------------------ |
| ![1570413615502](assets/1570413615502.png) |



2、JSON输入配置

| json输入配置图                             |
| ------------------------------------------ |
| ![1570413600128](assets/1570413600128.png) |



3、表输出配置

| 表输出配置                                 |
| ------------------------------------------ |
| ![1570413710722](assets/1570413710722.png) |





####  插入更新

* 插入更新就是把数据库已经存在的记录与数据流里面的记录进行比对
  * 如果不同就进行更新
  * 如果记录不存在，则会插入数据

需求：

- 从`资料\kettle测试数据\user_new.json` 中读数据，并插入或更新到mysql数据库的 t_user_1 表中

操作步骤：

1、拖入一个 输入/JSON输入组件，一个 输出/插入更新组件、连接两个组件

2、配置 JSON输入组件

3、配置 插入更新 组件

4、启动执行

具体实现：

1、拖入一个 输入/JSON输入组件，一个 输出/插入更新组件、连接两个组件

| 组件配置图                                 |
| ------------------------------------------ |
| ![1570415012710](assets/1570415012710.png) |



2、配置 JSON输入组件

| json输入组件                               |
| ------------------------------------------ |
| ![1570415026854](assets/1570415026854.png) |



3、配置 插入更新 组件

| 插入更新组件                               |
| ------------------------------------------ |
| ![1570415064377](assets/1570415064377.png) |



4、启动执行



####  删除

需求：

- 从mysql数据库 t_user_1 表中删除指定id为 492456198712198000 的数据

操作步骤：

1、拖入一个 输入/自定义常量数据、输出/删除 组件

2、连接两个组件

| 组件图                                              |
| --------------------------------------------------- |
| <img src="assets/1569672397419.png" align="left" /> |



3、配置自定义常量数据组件

| 自定义常量组件                             |
| ------------------------------------------ |
| ![1569672439446](assets/1569672439446.png) |





4、配置删除组件

| 配置删除组件                                      |
| ------------------------------------------------- |
| <img src="assets/1569672509885.png" align="left"> |



##  Kettle整合大数据平台

###  Kettle整合Hadoop

#### Hadoop环境准备

1、查看hadoop的文件系统

- 通过浏览器访问

```html
http://node1:50070/
```

- 通过终端访问

```shell
hadoop fs -ls / # 查看文件
```

2、在hadoop文件系统中创建/hadoop/test目录

```shell
hadoop fs -mkdir -p /hadoop/test  
```

3、在本地创建1.txt

- vim 1.txt

```html
id,name
1,itheima
2,itcast
```

4、上传1.txt到hadoop文件系统的/hadoop/test目录

```shell
hadoop fs -put 1.txt /hadoop/test
```

#### kettle与hahoop环境整合

1、确保Hadoop的环境变量设置好HADOOP_USER_NAME为root

2、从hadoop下载核心配置文件

```shell
sz /export/servers/hadoop-2.6.0-cdh5.14.0/etc/hadoop/hdfs-site.xml
sz /export/servers/hadoop-2.6.0-cdh5.14.0/etc/hadoop/core-site.xml
```

文件会被下载到windows的下载目录

sz命令设置下载到windows的目录：

| linux下载文件到windows                                       |
| ------------------------------------------------------------ |
| ![image-20200205162500695](assets/image-20200205162500695.png) |
| ![image-20200205162639526](assets/image-20200205162639526.png) |



3、把hadoop核心配置文件(hdfs-site.xml和core-site.xml)放入kettle目录

```shell
data-integration\plugins\pentaho-big-data-plugin\hadoop-configurations\cdh514
```

4、修改 `data-integration\plugins\pentaho-big-data-plugin\plugin.properties`文件

- 修改plugin.properties

```shell
active.hadoop.configuration=cdh514
```

| plugin.propeties                                             |
| ------------------------------------------------------------ |
| ![image-20200205162801320](assets/image-20200205162801320.png) |

5、 创建Hadoop clusters

具体步骤如下：

| 创建hadoop cluster                                           |
| ------------------------------------------------------------ |
| ![image-20200205162933991](assets/image-20200205162933991.png) |
| ![image-20200205163018669](assets/image-20200205163018669.png) |
| ![image-20200205163201486](assets/image-20200205163201486.png) |



点击测试结果如图片右侧展示效果说明整合hadoop环境没有问题！！点击确定保存以上操作即可！！

查看链接是否保存：

| hadoop连接保存                                               |
| ------------------------------------------------------------ |
| ![image-20200205163327492](assets/image-20200205163327492.png) |

如果与hadoop链接没有保存后续是无法操作hadoop集群！！

####  Hadoop file input组件

Kettle在Big data分类中提供了一个Hadoop file input 组件用来从hdfs文件系统中读取数据。

| hadoop file input                                            |
| ------------------------------------------------------------ |
| ![image-20200205163533264](assets/image-20200205163533264.png) |





需求：

- 从Hadoop文件系统读取/hadoop/test/1.txt文件，把数据输入到Excel中。



实习步骤：

1、拖入以下组件

<img src="assets/clip_image017.png" align="left" style="border:1px solid #999"/>

2、配置Hadoop File Input组件

指定hdfs的目标路径：

| hadoop file input                                            |
| ------------------------------------------------------------ |
| ![image-20200205164223439](assets/image-20200205164223439.png) |



指定文件内容格式：

| 指定文件内容格式                                             |
| ------------------------------------------------------------ |
| ![image-20200109145528606](assets/image-20200109145528606.png) |



点击字段查看获取字段是否正确：

| 获取字段                                                     |
| ------------------------------------------------------------ |
| ![image-20200205164439030](assets/image-20200205164439030.png) |



配置excel输出组件：

| excel输出组件配置                                            |
| ------------------------------------------------------------ |
| ![image-20200205164529691](assets/image-20200205164529691.png) |



点击excel输出组件获取字段查看字段是否正确：

| 获取字段                                                     |
| ------------------------------------------------------------ |
| ![image-20200205164624756](assets/image-20200205164624756.png) |



启动转换任务：

| 执行任务                                                     |
| ------------------------------------------------------------ |
| ![image-20200205164718324](assets/image-20200205164718324.png) |
| ![image-20200205164746715](assets/image-20200205164746715.png) |
| 执行结果                                                     |
| ![image-20200205164818973](assets/image-20200205164818973.png) |



#### Hadoop file output组件

Kettle在Big data分类中提供了一个Hadoop file output 组件用来向hdfs文件系统中保存数据

| hadoop file output组件                                       |
| ------------------------------------------------------------ |
| ![image-20200205163533264](assets/image-20200205163533264-1580983912463.png) |



需求：

- 读取 user.json 把数据写入到hdfs文件系统的的/hadoop/test/2.txt中。

实现步骤：

1、拖入以下组件

| 组件配置                                                     |
| ------------------------------------------------------------ |
| ![image-20200205165008109](assets/image-20200205165008109.png) |

2、配置 JSON 输入组件

| 指定json文件的路径                                           |
| ------------------------------------------------------------ |
| ![image-20200205165120462](assets/image-20200205165120462.png) |
| 配置json input组件读取的字段                                 |
| ![image-20200205165251370](assets/image-20200205165251370.png) |







3、配置Hadoop file output组件

| 指定hdfs目标路径                                             |
| ------------------------------------------------------------ |
| ![image-20200205165351594](assets/image-20200205165351594.png) |
| 指定源文件的属性信息：                                       |
| ![image-20200205165542753](assets/image-20200205165542753.png) |
| ![image-20200205165629082](assets/image-20200205165629082.png) |



- 问题

![image-20200205165908948](../../../../1上课共享资料/day01/1.讲义/assets/image-20200205165908948.png)

错误：用户没有权限

解决：

```shell
# 修改权限
  hadoop fs -chmod -R 777  /
```

### Kettle整合Hive

####  初始化数据

1、连接hive

| beeline连接hive                  |
| -------------------------------- |
| ![img](assets/clip_image039.jpg) |



2、创建并切换数据库

```sql
create database test;
use test;
```



3、创建表

```sql
create table a(
	a int,
    b int
)
row format delimited fields terminated by ',' stored as TEXTFILE;
show tables;
```



4、创建数据文件

```shell
vim a.txt
1,11
2,22
3,33
```



5、 从文件加载数据到表

```sql
load data local inpath '/root/a.txt' into table a;
```



6、查询表

```sql
select * from a;
```

#### kettle与Hive整合

1、从虚拟机下载Hadoop的jar包

```shell
sz /export/servers/hadoop-2.6.0-cdh5.14.0/share/hadoop/common/hadoop-common-2.6.0-cdh5.14.0.jar
```

2、把jar包放置在\data-integration\lib目录下

| 上传到lib目录下                                              |
| ------------------------------------------------------------ |
| ![image-20200205170115700](assets/image-20200205170115700.png) |



3、重启kettle，重新加载生效

关掉之前打开的kettle重新启动！！

#### 从hive中读取数据

- hive数据库是通过jdbc来进行连接，可以通过表输入控件来获取数据。

需求：

- 从hive数据库的test库的a表中获取数据，并把数据保存到Excel中。

实现步骤：

1、设计一下kettle组件结构

选择输入文件夹内的表输入组件：

| 表输入组件                                                   |
| ------------------------------------------------------------ |
| ![image-20200205170433530](assets/image-20200205170433530.png) |
| <img src="assets/clip_image047.png" align="left" style="border:1px solid #999"/> |



2、配置表输入组件

| 新建hivejdbc连接：                                           |
| ------------------------------------------------------------ |
| ![image-20200205170958301](assets/image-20200205170958301.png) |
| ![image-20200205171055461](assets/image-20200205171055461.png) |
| 配置excel输出组件                                            |
| ![image-20200205171150064](assets/image-20200205171150064.png) |
| ![image-20200205171210756](assets/image-20200205171210756.png) |



#### 把数据保存到hive数据库

hive数据库是通过jdbc来进行连接，可以通过表输出控件来保存数据。

需求：

- 从Excel资料\02.kettle测试数据\01.用户数据源\file_user.xls中读取数据，把数据保存在hive数据库的test数据库的t_user表。

实现步骤：

1、设计如下kettle组件结构

| 组件配置图                                                   |
| ------------------------------------------------------------ |
| <img src="assets/clip_image054.png" align="left" style="border:1px solid #999"/> |



2、配置 Excel输入组件

| excele输入组件                                               |
| ------------------------------------------------------------ |
| ![image-20200205171342879](assets/image-20200205171342879.png) |
| 查看excel解析字段是否正确                                    |
| ![image-20200205171512191](assets/image-20200205171512191.png) |



2、配置表输出组件

| 表输出组件                                                   |
| ------------------------------------------------------------ |
| ![image-20200227152851902](assets/image-20200227152851902.png) |
| 获取流中的字段                                               |
| ![image-20200205173951319](assets/image-20200205173951319.png) |
| ![image-20200205174959098](assets/image-20200205174959098.png) |
| 验证                                                         |
| ![image-20200205175138955](assets/image-20200205175138955.png) |



#### 执行Hive的HiveSQL语句

Kettle中可以执行Hive的HiveSQL语句，使用作业的SQL脚本。

需求：

- 聚合查询weblogs表（以IP和年月分组统计的PV数据），同时建立一个新表保存查询数据。

**准备hive表**

在hive的test数据库下创建weblogs表：

```sql
 CREATE TABLE `weblogs`(                           
  `client_ip` string,                              
  `full_request_date` string,                      
  `day` string,                                    
  `month` string,                                  
  `month_num` int,                                 
  `year` string,                                   
  `hour` string,                                   
  `minute` string,                                 
  `second` string,                                 
  `timezone` string,                               
  `http_verb` string,                              
  `uri` string,                                    
  `http_status_code` string,                       
  `bytes_returned` string,                         
  `referrer` string,                               
  `user_agent` string) 
  row format delimited fields terminated by '\t' stored as textfile;
```

导入资料\资料\02.kettle测试数据\hive-weblogs\下的数据

```shell
load data local inpath '/root/weblogs_parse.txt' into table weblogs;
```

验证数据

```sql
select * from test.weblogs limit 5;
```



实现步骤：

1、设计如下作业组件结构

| 设计如下作业组件结构                                         |
| ------------------------------------------------------------ |
| ![image-20200205175216309](assets/image-20200205175216309.png) |
| ![image-20200205175313672](assets/image-20200205175313672.png) |
| ![img](assets/clip_image069.png)                             |





2、配置SQL组件

| 配置sql组件                                                  |
| ------------------------------------------------------------ |
| ![image-20200205175421659](assets/image-20200205175421659.png) |
| ![image-20200205175421659](assets/image-20200205175421659.png) |



3、测试数据是否生成

| 验证数据                                                     |
| ------------------------------------------------------------ |
| ![image-20200205175518614](assets/image-20200205175518614.png) |



## kettle常用其它组件

###  Kettle转换组件

* 转换是ETL的T，T就是Transform清洗、转换

* ETL三个部分中，T花费时间最长,是“一般情况下这部分工作量是整个ETL的2/3

  | kettle转换组件                                      |
  | --------------------------------------------------- |
  | <img src="assets/1569672821580.png" align="left" /> |

  



####  值映射

* 值映射就是把字段的一个值映射成其他的值
* 在数据质量规范上使用非常多，比如很多系统对应性别gender字段的定义不同
  * 系统1：1 男、2女
  * 系统2：f 男、m 女
  * 数据仓库统一为：male 男、female女



需求：

- 从user.json 中读取数据，并把gender列
  - 0 -> 男
  - 1 -> 女
  - 2 -> 保密
- 写入到Excel文件



实现步骤：

1、拖入一个 JSON输入组件、一个值映射转换组件、一个Excel输出组件，连接三个组件

2、配置JSON输入组件

3、配置值映射转换组件

4、配置Excel输出组件



具体实现：

1、拖入一个 JSON输入组件、一个值映射转换组件、一个Excel输出组件，连接三个组件

| 组件配置图                                 |
| ------------------------------------------ |
| ![1570415691415](assets/1570415691415.png) |



2、配置JSON输入组件

3、配置值映射转换组件

| 值映射组件                                          |
| --------------------------------------------------- |
| <img src="assets/1570415717712.png" align="left" /> |

4、配置Excel输出组件



####  增加序列

* 增加序列就是给数据流增加一个序列字段

  | 增加序列                                            |
  | --------------------------------------------------- |
  | <img src="assets/1569674394571.png" align="left" /> |

  



需求：

* 从 user.json 读取数据，并添加序列，把数据保存到Excel

实现步骤：

1、拖入JSON输入组件、增加序列组件、Excel输出组件，并连接三个组件

| 组件配置图                                 |
| ------------------------------------------ |
| ![1570416388533](assets/1570416388533.png) |



2、配置JSON Input组件

3、配置增加序列组件

| 配置增加序列                                        |
| --------------------------------------------------- |
| <img src="assets/1570416417532.png" align="left" /> |



4、配置Excel输出组件



#### 字段选择

* 字段选择是从数据流中选择字段、改变名称、修改数据类型

需求：

- 从 user.json 中读取数据
- 移除birthday和register_date
- 把phone列名改为telephone，id列名改为key，gender列名改为sex
- 输出到Excel文件中



实现步骤：
1、拖入 JSON输入 组件、字段选择组件、Excel输出组件

| 组件配置图                                 |
| ------------------------------------------ |
| ![1570416693317](assets/1570416693317.png) |

2、配置输入、字段选择、输出组件

| 配置输入，字段选择，输出组件               |
| ------------------------------------------ |
| ![1570416732572](assets/1570416732572.png) |



### Kettle流程控件

- 流程主要用来控制数据流程和数据流向

####  switch case

- switch/case组件让数据流从一路到多路。

  | switch case                                                  |
  | ------------------------------------------------------------ |
  | <img src="assets/1569750489382.png" align="left" style="border:1px solid #999" /> |

  





需求：

- 从 user.json 输入读取数据，按sex进行数据分类，把女性、男性、保密分别保存不同的Excel文件里面。
  - 0表示男性
  - 1表示女性
  - 2表示保密

实现步骤：

1、拖入 JSON输入组件，switch/case组件，三个Excel输出组件

| 组件配置图                                               |
| -------------------------------------------------------- |
| ![1570429015330](assets/1570429015330-1570494615088.png) |



2、配置 switch/case 组件

| 配置 switch/case 组件                                    |
| -------------------------------------------------------- |
| ![1570429033924](assets/1570429033924-1570494615088.png) |





#### 过滤记录

过滤记录让数据流从一路到两路。

| 过滤记录                                                     |
| ------------------------------------------------------------ |
| <img src="assets/1569750782651.png" align="left" style="border:1px solid #999"> |





需求：

- 从 user.json 读取数据，分离出 年龄 大于等于25，小于25的数据，分别保存到不同的Excel文件



实现步骤：

1、拖入 JSON输入组件、过滤记录组件、两个Excel组件，并连接各个组件

| 组件配置图                                               |
| -------------------------------------------------------- |
| ![1570345185863](assets/1570345185863-1570494615067.png) |



2、配置过滤记录组件

| 配置过滤组件                                        |
| --------------------------------------------------- |
| <img src="assets/1570345267807.png" align="left" /> |





###  Kettle连接控件

#### 笛卡尔积

| 笛卡尔积                                            |
| --------------------------------------------------- |
| <img src="assets/1570358308997.png" align="left" /> |





需求：

- 从Excel读取两位和三位数，完成两位数和三位数的组合（笛卡尔积）,把结果保存在Excel

实现步骤：

1、设计转换结构

| 转换结构图                                               |
| -------------------------------------------------------- |
| ![1570430997052](assets/1570430997052-1570494615092.png) |



2、配置记录关联（笛卡尔积组件）

| 配置记录关联                                             |
| -------------------------------------------------------- |
| ![1570431011049](assets/1570431011049-1570494615092.png) |

#### 记录集连接

- 记录集连接类似数据库的左连接、右连接、内连接、外连接。
- 在进行记录集连接之前，应该要对记录集进行**排序**。



需求：

- 从Excel中读取employees和departments数据，进行内关联，左关联，右关联，全关联，把数据保存到Excel



实现步骤：

1、设计以下组件图

| 组件转换图                                               |
| -------------------------------------------------------- |
| ![1570347116451](assets/1570347116451-1570494615089.png) |



2、配置记录集连接组件

| 配置记录集连接组件                                  |
| --------------------------------------------------- |
| <img src="assets/1570347152702.png" align="left" /> |



##  Kettle作业和参数

###  Job（作业）

大多数ETL项目都需要完成各种各样的操作，例如：

* 如何传送文件
* 验证数据库表是否存在，等等

而这些操作都是按照一定顺序完成，Kettle中的作业可以串行执行转换来处理这些操作。

| 配置作业                                   |
| ------------------------------------------ |
| ![1570350109942](assets/1570350109942.png) |





####  Job Item（作业项）

**作业项**是作业的基本构成部分。如同转换的组件，作业项也可以用**图标的方式**展示。

| 作业展示图                                 |
| ------------------------------------------ |
| ![1570365028328](assets/1570365028328.png) |





* 作业顺序执行作业项，必须定义一个**起点**
* 有一个「start」的作业项专门用来定义起点
* 一个作业只能定一个开始作业项



####  Job Hop（作业跳）

Job Hop是作业项之间的连接线，定义了作业的执行路径，作业里每个作业项的不同运行结果决定了作业的不同执行路径。以下为 Job Hop的几种执行方式：

1、无条件执行

* 不论上一个作业项执行成功还是失败，下一个作业项都会执行

* 蓝色的连接线，上面有一个锁的图标

  | 无条件执行                                          |
  | --------------------------------------------------- |
  | <img src="assets/1570365475738.png" align="left" /> |

  

2、当运行结果为真时执行

* 当上一个作业项的执行结果为真时，执行下一个作业项

* 通常在需要无错误执行的情况下使用

* 绿色的连接线，上面有一个对钩号的图标。

  | 结果为真执行                                        |
  | --------------------------------------------------- |
  | <img src="assets/1570365533247.png" align="left" /> |

  



3、当运行结果为假时执行

* 当上一个作业项的执行结果为假或者没有成功执行时，执行下一个作业项
* 红色的连接线，上面有一个红色的停止图标

| 当结果为假时执行                                    |
| --------------------------------------------------- |
| <img src="assets/1570365585789.png" align="left" /> |



####  作业示例

需求：

* 先从 `资料\作业数据源\01Excel输入.xlsx`读取数据，保存到Excel
* 再从 `资料\作业数据源\01文本文件输入.txt` 文本文件中读取数据，保存到Excel
* 启动作业执行
  * 执行错误，显示执行错误消息框
  * 执行成功，显示执行成功消息框



实现步骤：

1、设计转换结构1（从Excel读取数据，保存到Excel）

| 从Excel读取数据，保存到Excel                        |
| --------------------------------------------------- |
| <img src="assets/1570366773214.png" align="left" /> |
| 从文本文件中读取数据，保存到Excel                   |
| <img src="assets/1570366794701.png" align="left"/>  |



3、设计作业结构（先执行转换结构1、再执行转换结构2）

| 作业结构图                                 |
| ------------------------------------------ |
| ![1570366831147](assets/1570366831147.png) |



4、运行测试

5、错误测试

| 将第一个转换结构直接终止，并配置抛出一个错误        |
| --------------------------------------------------- |
| ![1570366711199](assets/1570366711199.png)          |
| <img src="assets/1570366744849.png" align="left" /> |



### 2 参数

####  参数的使用

对于ETL参数传递是一个很重要的环节，因为参数的传递会涉及到业务数据是如何抽取

####  转换命名参数

* 转换命名参数就是在转换内部定义的变量，作用范围是在转换内部

* 在转换的空白处双击左键，在转换属性中能看到

* 可以在表输入 SQL语句中使用 ${变量名} 或者 %%变量名%% 直接引用

  | 转换命名参数                               |
  | ------------------------------------------ |
  | ![1570368608385](assets/1570368608385.png) |

  

需求：

* 设置转换命名参数 default_province = 北京市
* 从t_user表中获取数据，满足条件 province=default_province，后续不要执行任何操作

实现步骤：

1、设计以下转换组件结构图

| 转换结构图                                          |
| --------------------------------------------------- |
| <img src="assets/1570368865960.png" align="left" /> |



2、配置转换命名参数

| 设置转换命名参数                           |
| ------------------------------------------ |
| ![1570369292238](assets/1570369292238.png) |



3、配置表输入组件

| 配置表输入组件                             |
| ------------------------------------------ |
| ![1570369270496](assets/1570369270496.png) |



4、执行转换

| 执行转焕                                            |
| --------------------------------------------------- |
| <img src="assets/1570369180659.png" align="left" /> |



## Kettle Linux部署

| kettle linux部署                           |
| ------------------------------------------ |
| ![1570583989196](assets/1570583989196.png) |



###  Linux安装Kettle

1、用File Zilla将kettle上传到Linux服务器，并解压缩

2、在命令行执行 

```shell
./pan.sh -version
./kitchen.sh -version
```

3、如果能够看到以下输出，表示kettle可以正确运行

```shell
2019/10/09 08:49:09 - Pan - Kettle version 8.2.0.0-342, build 8.2.0.0-342, build date : 2018-11-14 10.30.55
2019/10/09 08:49:09 - Pan - Start of run.
ERROR: No repository provided, can't load transformation.
```



```shell
2019/10/09 08:13:21 - Kitchen - Kettle version 8.2.0.0-342, build 8.2.0.0-342, build date : 2018-11-14 10.30.55
2019/10/09 08:13:21 - Kitchen - Start of run.
ERROR: Kitchen can't continue because the job couldn't be loaded.
```



4、配置环境变量

```shell
# KETTLE
export KETTLE=/export/softwares/data-integration
export PATH=${KETTLE}:$PATH
```





###  Pan——转换执行引擎

pan.sh可以用来在服务器中执行一个转换

pan.sh的命令行参数:

```shel
-version：显示版本信息
-file: 指定要运行的转换文件（XML文件）
-level: 设置日志级别(Basic,Detailed,Debug,Rowlevel,Error,Nothing)
-log: 指定日志文件
-param:key=value （该参数可以指定多个）覆盖之前指定的默认的命名参数
```



需求：

* 在Linux中，将 /root/kettle/user.json 数据抽取到 /root/kettle/user.xls 表格中



实现步骤：

1、在 windows 中开发转换，将 json数据抽取装载到 user.xls文件中

2、抽取路径参数，通过命令行指定 json数据文件路径，指定 user.xls 文件路径

| 设置转换命名参数                           |
| ------------------------------------------ |
| ![1570583639139](assets/1570583639139.png) |
| ![1570583639139](assets/1570583639139.png) |
| ![1570583672960](assets/1570583672960.png) |
| ![1570583692029](assets/1570583692029.png) |





3、将数据文件上传到 /root/kettle 目录

4、上传转换文件、json数据文件到Linux服务器

5、使用 pan.sh 执行转换

```shell
pan.sh -file 8.transform_param.ktr -level Basic -param:input=/root/kettle/user.json -param:output=/root/kettle/output_user
```



### Kitchen——作业执行引擎

在Linux中，可以使用 kitchen.sh 来执行作业

需求：

* 执行JSON数据抽取到Excel中

实现步骤：

1、在windows中开发作业

| 作业配置图                                 |
| ------------------------------------------ |
| ![1574402751092](assets/1574402751092.png) |





2、配置转换组件

| 引入之前定义好的转换任务                   |
| ------------------------------------------ |
| ![1574402829839](assets/1574402829839.png) |





3 windows本地测试执行

4 、修改转换中的路径参数改为用变量来接收

| windows测试                                                  |
| ------------------------------------------------------------ |
| ![image-20200206181718257](assets/image-20200206181718257.png) |



5、配置作业命名参数

| 作业的命名参数                             |
| ------------------------------------------ |
| ![1570583550498](assets/1570583550498.png) |



6、启动测试执行

6、上传JOB文件到Linux服务器的`/root/kettle/`目录

7、使用kitchen.sh执行作业

```shell
kitchen.sh -file job_transform.kjb -level Basic -param:input=/root/kettle/user.json -param:output=/root/kettle/output_user
```

# 